{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b79c40d",
   "metadata": {},
   "source": [
    "#### Types of data:-  \n",
    "    1-Structured Data (like sql)  \n",
    "    2-Semi-structured Data (like csv, xml, json)  \n",
    "    3-Unstructured Data (like pdf, emails)  \n",
    "    4-Binary Data (like mp3, photos)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4505e993",
   "metadata": {},
   "source": [
    "###### Data Mining- means preprocessing and extraction some knowledge from the data.  \n",
    "###### Big Data- means that we have a lot of data ,alot of variable ,that can't run on one computer it is too much data computer to hold. so we run it on cloud such as Azure, google cloud.  \n",
    "###### Data Pipeline- is essentially a pipeline that allows us to flow from that unknown large amount of data to a pipeline that extracts and data to a more useful form.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb94d8f",
   "metadata": {},
   "source": [
    "#### Kafka, hadoop, amazon S3, azure data lake\n",
    "these are programs thta have been built by engineers to hold large amout of data. (data lakes) \n",
    "#### amazon athena, google big query, amazon red shift\n",
    "these are data warehouses that allows engineers to make queries or analyze this structured data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eaef07",
   "metadata": {},
   "source": [
    "###### As a machine learning engineer we generally use data lakes as more amount of data is better..  \n",
    "##### Data warehouses are used by business intelligence people or business/data analyst to make visualization or analyze data as warehouses have more structured data..  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3082e3",
   "metadata": {},
   "source": [
    "1.A software engineer, app developer, etc build programs and apps that releases data.  \n",
    "\n",
    "2.then a Data engineer build this piping and pipeline for us to ingest and store data so that it can be accessed by rest of the business.  \n",
    "\n",
    "3.next Data Scientists use that data lake to extract information and deliver some sort of business value.  \n",
    "\n",
    "4.finally we have Data Analysts or business intelligence to use data warehouse or structured data to again derive business values...  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230aab4a",
   "metadata": {},
   "source": [
    "##### Three main tasks that a Data Engineer do !!!\n",
    "1- They build  what we call ETL (extract, transform, load) pipeline. They extract the data transform it  into a useful ofrm and load it into something called data warehouse so that data can be used by the rest of the company. for this they use programming language like python, go skalla, java...  \n",
    "\n",
    "2- They build analysis tools. So a data engineer allows data scientist & analysts to analyze the data and make sure that the system they have put in place is running correctly...  \n",
    "\n",
    "3- Finally their third main task is to maintain the data warehouse and data lakes i.e making sure everythiung is accessible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362dbda",
   "metadata": {},
   "source": [
    "#### Types of Databases:-\n",
    "We have different databases because we have different needs with data.  \n",
    "There are three main categories of Database:-  \n",
    "    1-Relational database  \n",
    "    2-NoSQL  \n",
    "    3-NewSQL  \n",
    "We also have things like computational databases, OSTP, OLAP....etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45090be0",
   "metadata": {},
   "source": [
    "<img src= \"DB_landscape.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bcd81f",
   "metadata": {},
   "source": [
    "https://blog.yugabyte.com/a-primer-on-acid-transactions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da821c",
   "metadata": {},
   "source": [
    "https://techdifferences.com/difference-between-oltp-and-olap.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7274d",
   "metadata": {},
   "source": [
    "A database is a collection of data (can be numbers, dates, password hashes, user information.   \n",
    "\n",
    "Databases allow us to organize this data in a way that is useful to us, and it makes data management easy.  \n",
    "\n",
    "Database Management System, or DBMS is a collection of programs which allows us to access databases and work with data, and it also allows control access to database users.  \n",
    "\n",
    "There are 2 main types:-  \n",
    "1) Relational databases- postgrads, oracle, sql server, mysql etc  \n",
    "2) NoSQL/ Non Relational database- couchDB, hypertable, redis, mongoDB, etc  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5380a",
   "metadata": {},
   "source": [
    "<img src=\"5ON0WC8.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbcb48",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713de8d",
   "metadata": {},
   "source": [
    "## hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4cfbd",
   "metadata": {},
   "source": [
    "Data bases like MySQL are inefficient or unable to hold large amount of data.  \n",
    "\n",
    "Hadoop is an open source distributed processing framework which allows us to do data processing and storage for big data.   \n",
    "\n",
    "Hadoop was a solution that allowed all these companies with petabytes of data to store this information in Hadoop.  \n",
    "\n",
    "The popularity of Hadoop was because of two big things.  \n",
    "\n",
    "One was HDFC, that is the Hadoop distributed file system.  \n",
    "Hadoop was able to store so much data because of this, a file system just like you have on your computer that allowed it to store files on multiple computers.\n",
    "So data is stored across different physical computers.  \n",
    "\n",
    "Other was map reduced because once we store data, we need to perform some jobs, some processing on that data, right.  \n",
    "\n",
    "And map reduce and Hadoop allowed us to perform jobs against this data that we had in a data lake using languages like Java or Python.    \n",
    "  \n",
    "  \n",
    "Now, Map Reduce isn't used as often anymore because we have something called Apache Spark that's actually a little bit faster.    \n",
    "\n",
    "And a lot of other tools started popping up around Hadoop tools such as Hive. Hive makes your Hadoop cluster.  \n",
    "\n",
    "Remember, data engineers usually don't work on systems that are customer facing.  \n",
    "That is, they don't build databases that you would use with your app or with an webapp.  \n",
    "Instead, they build the systems behind the scenes that ingests, collects, runs, ETL jobs, that is extract, transform, load on data.  \n",
    "  \n",
    "Hadoop is still very popular and very capable when it comes to large data, petabytes of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2995f5",
   "metadata": {},
   "source": [
    "## Apache spark & Apache flink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d431fdf",
   "metadata": {},
   "source": [
    "before Apache Spark, usually people used Hadoop map produce to process everything.  \n",
    "\n",
    "Spark came along and actually improved the map produce and Hadoop by doing something called in-memory processing, which essentially allowed Spark to run processing jobs much, much faster than mass produce.  \n",
    "\n",
    "So Pache Spark became really, really popular, and it's probably the go to batch processing framework.\n",
    "\n",
    "So if you want to process a lot of data, well, you can use Hadoop to store that data and HDFC and use Apache Spark to run ETEL jobs like extract, transform, load to clean and transform that data.    \n",
    "\n",
    "Now, up until now, we had batch processing, which essentially means give me a chunk of data and I'll process this data over a bit of time, usually you'd run a batch processing job at the end of the night and in the morning, after a couple of hours, the job is done.  \n",
    "\n",
    "But as of a few years ago, the idea of real time processing started to happen.  \n",
    "\n",
    "Things like spark streaming came out, which almost made real time processing possible.  \n",
    "\n",
    "That is, every time data comes in, you process that data in this space.  \n",
    "\n",
    "Apache Flink really took on the charge. It offers real time stream processing now, although Spark is still popular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42a49b",
   "metadata": {},
   "source": [
    "## Kafka & Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbaec4b",
   "metadata": {},
   "source": [
    "So we learned that as a data engineer, you collect data, you ingest data, and then you store it in a data lake, often a Hadoop cluster.  \n",
    "\n",
    "Now, this idea of a data leak can also be used with something like AYSO, S3 Object Storage.  \n",
    "\n",
    "The idea of stream processing is pretty complicated, but one of the main tools that's being used right now for stream processing is something called Kafka.  \n",
    "\n",
    "Kafka is a distributed streaming platform. It allows us to read and write streams of data like messaging system. It allows us to process and it allows us to store data as well.  \n",
    "\n",
    "But as you can see what Kafka does, it allows us to receive messages and pass it on to different places.  \n",
    "\n",
    "So you'll often see we use Apache, Kafka to receive all these messages, logs and data from all these sources, we collect them in a central location, and then Kafka can pass it on to different locations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8f375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
